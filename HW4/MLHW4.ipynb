{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxuVdRvdBSFi"
      },
      "source": [
        "# HW4 RNN\n",
        "Task: Sentiment classification on Twitter comments.\n",
        "\n",
        "Goal of this homework:\n",
        "*   Get familiar with the recurrent neural network.\n",
        "*   Learn how to deal with text data\n",
        "\n",
        "TA: Chih-Kai, Yang (b08202033@ntu.edu.tw)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5YkPW-dPD-k"
      },
      "outputs": [],
      "source": [
        "# !gdown --id \"1cwPgbbAMNPZ9nCoyOW2WuavimYymCKKy\" --output DATASET.zip\n",
        "# !unzip DATASET.zip\n",
        "!pip install wandb\n",
        "import wandb\n",
        "!pip install ipdb\n",
        "import ipdb\n",
        "wandb.login"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.dropbox.com/s/0abp8vvib4j1gjw/HW4_dataset.zip\n",
        "!mv HW4_dataset.zip DATASET.zip\n",
        "!unzip DATASET.zip"
      ],
      "metadata": {
        "id": "Xq9CJZiSgJ9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "1VX5NXp4WbEi",
        "outputId": "6895e8d8-840f-45dd-c9ac-08ded4adf0fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading word2vec model ...\n",
            "total words: 20427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:176: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_42b9bbc2-f922-4562-a141-053d9dd85a03\", \"pred_colab.csv\", 400010)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\"\"\"********************************************* \n",
        "  Import packages.\n",
        " *********************************************\"\"\"\n",
        "# other library\n",
        "import os\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# PyTorch library\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Self-defined\n",
        "import argparse\n",
        "from tqdm import trange\n",
        "import wandb\n",
        "import ipdb\n",
        "import re\n",
        "# ipdb.set_trace(context=5)\n",
        "SEED = 1124 # Set your lucky number as the random seed\n",
        "# root_path = './drive/MyDrive/Colab Notebooks/MLHW4/model/'  # In colab version\n",
        "# MODEL_DIR = root_path + 'RNN/model'  # In colab version\n",
        "MODEL_DIR = './model'\n",
        "\n",
        "\"\"\"********************************************* \n",
        "  Basic setup of hyperparameters\n",
        " *********************************************\"\"\"\n",
        "EPOCH_NUM = 100\n",
        "lr = 1e-4\n",
        "BATCH_SIZE = 256\n",
        "OPTIMIZER = 'adam'\n",
        "\n",
        "weight_d = 1e-3\n",
        "momentum = 0.9\n",
        "CHECKPOINT = '80.21772'#MODEL_DIR + '_55.33854.pth'#MODEL_DIR + '_55.33854.pth'\n",
        "gamma = 0.8\n",
        "step = 20\n",
        "\n",
        "MAX_POSITIONS_LEN = 100\n",
        "w2v_dim = 250\n",
        "embedding_dim = 250       # Use config on web\n",
        "net_hidden_dim = 150    # Use config on web\n",
        "net_num_layers = 3\n",
        "dropout = 0.5\n",
        "header_hidden_dim = 150\n",
        "\n",
        "mode = 'test'\n",
        "WANDB = False\n",
        "DATA_AUG = False\n",
        "SCHEDULER = False\n",
        "\n",
        "\"\"\"********************************************* \n",
        "  Do not changed\n",
        " *********************************************\"\"\"\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# w2v_config = {'path': './drive/MyDrive/Colab Notebooks/MLHW4/model/w2v/w2v_local_' + str(w2v_dim) + '_parse_text_cbow.model', 'dim': w2v_dim} # In colab\n",
        "w2v_config = {'path': './w2v_local_' + str(w2v_dim) + '_parse_text_cbow.model', 'dim': w2v_dim} # In submission version\n",
        "# net_config = {'hidden_dim': net_hidden_dim, 'num_layers': net_num_layers, 'bidirectional': False, 'fix_embedding': True}\n",
        "net_config = {'embedding_dim': embedding_dim, 'hidden_dim': net_hidden_dim, 'num_layers': net_num_layers, 'bidirectional': False, 'fix_embedding': True}\n",
        "header_config = {'dropout': dropout, 'hidden_dim': header_hidden_dim}\n",
        "assert header_config['hidden_dim'] == net_config['hidden_dim'] or header_config['hidden_dim'] == net_config['hidden_dim'] * 2\n",
        "\n",
        "\n",
        "\"\"\"********************************************* \n",
        "  Self-defined\n",
        " *********************************************\"\"\"\n",
        "def wandb_update():\n",
        "    config = wandb.config\n",
        "    config.epochs = EPOCH_NUM\n",
        "    config.learning_rate = lr\n",
        "    config.batch_size = BATCH_SIZE\n",
        "    config.optimizer = OPTIMIZER\n",
        "\n",
        "    config.weight_d = weight_d\n",
        "    config.momentum = momentum\n",
        "    config.checkpoint = CHECKPOINT\n",
        "    config.gamma = gamma\n",
        "    config.step = step\n",
        "\n",
        "    config.max_position_len = MAX_POSITIONS_LEN\n",
        "    config.w2v_dim = w2v_dim\n",
        "    config.embedding_dim = embedding_dim\n",
        "    config.net_hidden_dim = net_hidden_dim\n",
        "    config.net_num_layers = net_num_layers\n",
        "    config.dropout = dropout\n",
        "    config.header_hidden_dim = header_hidden_dim\n",
        "\n",
        "    config.data_aug = DATA_AUG\n",
        "    config.scheduler = SCHEDULER\n",
        "\n",
        "\n",
        "\"\"\"********************************************* \n",
        "  Auxiliary functions and classes definition\n",
        " *********************************************\"\"\"\n",
        "def parsing_text(text):\n",
        "    # TODO: do data processing\n",
        "    # print(text)\n",
        "    text = text.split(' ')\n",
        "    at_someone = re.compile('^@')\n",
        "    http_str = re.compile('^http')\n",
        "    www_str = re.compile('^www')\n",
        "    hashtag_str = re.compile('^#')\n",
        "    text = [string for string in text if not re.match(at_someone, string)]  # Remove @.... string\n",
        "    text = [string for string in text if not re.match(http_str, string)]    # Remove http... string(url)\n",
        "    text = [string for string in text if not re.match(www_str, string)]     # Remove www... string(url)\n",
        "    text = [string for string in text if not re.match(hashtag_str, string)]     # Remove #... string(hashtag)\n",
        "    pass_str = ['.', '-', '&lt;', '&gt;', '&amp;', '&quot;', '!!!']         # Replace blacklist string to white space\n",
        "    for i in range(len(text)):\n",
        "        for j in range(len(pass_str)):\n",
        "            text[i] = text[i].replace(pass_str[j], ' ')\n",
        "    text = ' '.join(text)\n",
        "    # print(text)\n",
        "    return text\n",
        "\n",
        "# def load_train_label(path='./drive/MyDrive/Colab Notebooks/MLHW4/HW4_dataset/train.csv'): # In colab version\n",
        "def load_train_label(path='./HW4_dataset/train.csv'):\n",
        "    tra_lb_pd = pd.read_csv(path)\n",
        "    label = torch.FloatTensor(tra_lb_pd['label'].values)\n",
        "    idx = tra_lb_pd['id'].tolist()\n",
        "    text = [parsing_text(s).split(' ') for s in tra_lb_pd['text'].tolist()]\n",
        "    return idx, text, label\n",
        "\n",
        "# def load_train_nolabel(path='./drive/MyDrive/Colab Notebooks/MLHW4/HW4_dataset/train_nolabel.csv'): # In colab version\n",
        "def load_train_nolabel(path='./HW4_dataset/train_nolabel.csv'):\n",
        "    tra_nlb_pd = pd.read_csv(path)\n",
        "    text = [parsing_text(s).split(' ') for s in tra_nlb_pd['text'].tolist()]\n",
        "    return None, text, None\n",
        "\n",
        "# def load_test(path='./drive/MyDrive/Colab Notebooks/MLHW4/HW4_dataset/test.csv'): # In colab version\n",
        "def load_test(path='./HW4_dataset/test.csv'):\n",
        "    tst_pd = pd.read_csv(path)\n",
        "    idx = tst_pd['id'].tolist()\n",
        "    text = [parsing_text(s).split(' ') for s in tst_pd['text'].tolist()]\n",
        "    return idx, text\n",
        "\n",
        "class Preprocessor:\n",
        "    def __init__(self, sentences, w2v_config):\n",
        "        self.sentences = sentences\n",
        "        self.idx2word = []\n",
        "        self.word2idx = {}\n",
        "        self.embedding_matrix = []\n",
        "        self.build_word2vec(sentences, **w2v_config)\n",
        "        \n",
        "    def build_word2vec(self, x, path, dim):\n",
        "        if os.path.isfile(path):\n",
        "            print(\"loading word2vec model ...\")\n",
        "            w2v_model = Word2Vec.load(path)\n",
        "        else:\n",
        "            print(\"training word2vec model ...\")\n",
        "            w2v_model = Word2Vec(x, size=dim, window=5, min_count=2, workers=12, iter=2, sg=1)\n",
        "            print(\"saving word2vec model ...\")\n",
        "            w2v_model.save(path)\n",
        "            \n",
        "        self.embedding_dim = w2v_model.vector_size\n",
        "        for i, word in enumerate(w2v_model.wv.vocab):\n",
        "            #e.g. self.word2index['he'] = 1 \n",
        "            #e.g. self.index2word[1] = 'he'\n",
        "            #e.g. self.vectors[1] = 'he' vector\n",
        "            \n",
        "            self.word2idx[word] = len(self.word2idx)\n",
        "            self.idx2word.append(word)\n",
        "            self.embedding_matrix.append(w2v_model[word])\n",
        "        \n",
        "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
        "        self.add_embedding('<PAD>')\n",
        "        self.add_embedding('<UNK>')\n",
        "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
        "        \n",
        "    def add_embedding(self, word):\n",
        "        # 把 word 加進 embedding，並賦予他一個隨機生成的 representation vector\n",
        "        # word 只會是 \"<PAD>\" 或 \"<UNK>\"\n",
        "        vector = torch.empty(1, self.embedding_dim)\n",
        "        torch.nn.init.uniform_(vector)\n",
        "        self.word2idx[word] = len(self.word2idx)\n",
        "        self.idx2word.append(word)\n",
        "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)   \n",
        "        \n",
        "    def sentence2idx(self, sentence):\n",
        "        sentence_idx = []\n",
        "        for word in sentence:\n",
        "            if word in self.word2idx.keys():\n",
        "                sentence_idx.append(self.word2idx[word])\n",
        "            else:\n",
        "                sentence_idx.append(self.word2idx[\"<UNK>\"])\n",
        "        return torch.LongTensor(sentence_idx)\n",
        "    \n",
        "class TwitterDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, id_list, sentences, labels, preprocessor):\n",
        "        self.id_list = id_list\n",
        "        self.sentences = sentences\n",
        "        self.labels = labels\n",
        "        self.preprocessor = preprocessor\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if self.labels is None: return self.id_list[idx], self.preprocessor.sentence2idx(self.sentences[idx])\n",
        "        return self.id_list[idx], self.preprocessor.sentence2idx(self.sentences[idx]), self.labels[idx]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "    \n",
        "    def collate_fn(self, data):\n",
        "        id_list = torch.LongTensor([d[0] for d in data])\n",
        "        lengths = torch.LongTensor([len(d[1]) for d in data])\n",
        "        texts = pad_sequence(\n",
        "            [d[1] for d in data], batch_first=True).contiguous()\n",
        "     \n",
        "        if self.labels == None: \n",
        "            return id_list, lengths, texts\n",
        "        else:\n",
        "          labels = torch.FloatTensor([d[2] for d in data])\n",
        "          return id_list, lengths, texts, labels\n",
        "\n",
        "\n",
        "# train_idx, train_label_text, label = load_train_label('./drive/MyDrive/Colab Notebooks/MLHW4/HW4_dataset/train.csv')  # In colab version\n",
        "train_idx, train_label_text, label = load_train_label('./HW4_dataset/train.csv')\n",
        "preprocessor = Preprocessor(train_label_text, w2v_config)\n",
        "\n",
        "\n",
        "train_idx, valid_idx, train_label_text, valid_label_text, train_label, valid_label = train_test_split(train_idx, train_label_text, label, test_size=0.5)\n",
        "train_dataset, valid_dataset = TwitterDataset(train_idx, train_label_text, train_label, preprocessor), TwitterDataset(valid_idx, valid_label_text, valid_label, preprocessor)\n",
        "\n",
        "# test_idx, test_text = load_test('./drive/MyDrive/Colab Notebooks/MLHW4/HW4_dataset/test.csv')  # In colab version\n",
        "test_idx, test_text = load_test('./HW4_dataset/test.csv')\n",
        "# test_idx, test_text = load_test('./drive/MyDrive/Colab Notebooks/MLHW4/HW4_dataset/hwq5.csv')\n",
        "test_dataset = TwitterDataset(test_idx, test_text, None, preprocessor)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle = True, collate_fn = train_dataset.collate_fn, num_workers = 8)\n",
        "valid_loader = torch.utils.data.DataLoader(dataset = valid_dataset, batch_size = BATCH_SIZE, shuffle = False, collate_fn = valid_dataset.collate_fn, num_workers = 8)\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE, shuffle = False, collate_fn = test_dataset.collate_fn, num_workers = 8)\n",
        "\n",
        "\n",
        "\"\"\"********************************************* \n",
        "  Definition of RNN network\n",
        " *********************************************\"\"\"\n",
        "class Backbone(torch.nn.Module):\n",
        "    def __init__(self, embedding, embedding_dim, hidden_dim, num_layers, bidirectional, fix_embedding=True):\n",
        "        super(Backbone, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))\n",
        "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
        "        self.embedding.weight.requires_grad = False if fix_embedding else True  # 是否將embedding固定住，如果fix_embedding爲False，在訓練過程中，embedding也會跟着被訓練\n",
        "        \n",
        "        # self.net = torch.nn.RNN(embedding.size(1), hidden_dim, num_layers=num_layers, bidirectional=bidirectional, batch_first=True)\n",
        "        self.net = torch.nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)  # Use config on web\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        inputs = self.embedding(inputs)\n",
        "        x, _ = self.net(inputs)\n",
        "        return x\n",
        "    \n",
        "class Header(torch.nn.Module):\n",
        "    def __init__(self, dropout, hidden_dim):\n",
        "        super(Header, self).__init__()\n",
        "        # TODO: you should design your classifier module\n",
        "        self.classifier = torch.nn.Sequential(torch.nn.Linear(hidden_dim, 1),\n",
        "                            torch.nn.Sigmoid())\n",
        "        \n",
        "    @ torch.no_grad()\n",
        "    def _get_length_masks(self, lengths):\n",
        "        # lengths: (batch_size, ) in cuda\n",
        "        ascending = torch.arange(MAX_POSITIONS_LEN)[:lengths.max().item()].unsqueeze(0).expand(len(lengths), -1).to(lengths.device)\n",
        "        length_masks = (ascending < lengths.unsqueeze(-1)).unsqueeze(-1)\n",
        "        return length_masks\n",
        "    \n",
        "    def forward(self, inputs, lengths):\n",
        "        # the input shape should be (N, L, D∗H)\n",
        "        pad_mask = self._get_length_masks(lengths)\n",
        "        inputs = inputs * pad_mask\n",
        "        inputs = inputs.sum(dim=1)\n",
        "        out = self.classifier(inputs).squeeze()\n",
        "        return out\n",
        "\n",
        "\n",
        "\"\"\"********************************************* \n",
        "  Trainer\n",
        " *********************************************\"\"\"\n",
        "def train(train_loader, backbone, header, optimizer, criterion, device, epoch):\n",
        "\n",
        "    total_loss = []\n",
        "    total_acc = []\n",
        "    \n",
        "    for i, (idx_list, lengths, texts, labels) in enumerate(train_loader):\n",
        "        lengths, inputs, labels = lengths.to(device), texts.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        if not backbone is None:\n",
        "            inputs = backbone(inputs)\n",
        "        soft_predicted = header(inputs, lengths)\n",
        "        loss = criterion(soft_predicted, labels)\n",
        "        total_loss.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if WANDB:\n",
        "          wandb.log({\"train_loss\": np.mean(total_loss)})\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            hard_predicted = (soft_predicted >= 0.5).int()\n",
        "            correct = sum(hard_predicted == labels).item()\n",
        "            batch_size = len(labels)\n",
        "            acc = correct * 100 / len(labels)\n",
        "            total_acc.append(acc)\n",
        "\n",
        "            if WANDB:\n",
        "              wandb.log({\"lr\": optimizer.param_groups[0]['lr'],\n",
        "                          \"train_acc\": np.mean(total_acc),})\n",
        "    backbone.train()\n",
        "    header.train()\n",
        "    return np.mean(total_loss), np.mean(total_acc)\n",
        "\n",
        "def valid(valid_loader, backbone, header, criterion, device, epoch):\n",
        "    backbone.eval()\n",
        "    header.eval()\n",
        "    with torch.no_grad():\n",
        "        total_loss = []\n",
        "        total_acc = []\n",
        "        \n",
        "        for i, (idx_list, lengths, texts, labels) in enumerate(valid_loader):\n",
        "            lengths, inputs, labels = lengths.to(device), texts.to(device), labels.to(device)\n",
        "\n",
        "            if not backbone is None:\n",
        "                inputs = backbone(inputs)\n",
        "            soft_predicted = header(inputs, lengths)\n",
        "            loss = criterion(soft_predicted, labels)\n",
        "            total_loss.append(loss.item())\n",
        "            \n",
        "            hard_predicted = (soft_predicted >= 0.5).int()\n",
        "            correct = sum(hard_predicted == labels).item()\n",
        "            acc = correct * 100 / len(labels)\n",
        "            total_acc.append(acc)\n",
        "\n",
        "            if WANDB:\n",
        "                wandb.log({\"val_loss\": np.mean(total_loss),\n",
        "                            \"val_acc\": np.mean(total_acc),})\n",
        "    backbone.train()\n",
        "    header.train()\n",
        "    return np.mean(total_loss), np.mean(total_acc)\n",
        "\n",
        "            \n",
        "def run_training(train_loader, valid_loader, backbone, header, epoch_num, lr, device, model_dir): \n",
        "    def check_point(backbone, header, loss, acc, model_dir):\n",
        "        torch.save(backbone, model_dir + \"_backbone_\" + str(round(acc, 5)) + '.pth')\n",
        "        torch.save(header, model_dir + \"_header_\" + str(round(acc, 5)) + '.pth')\n",
        "    def is_stop(loss, acc):\n",
        "        # TODO\n",
        "        return False\n",
        "    \n",
        "    if backbone is None:\n",
        "        trainable_paras = header.parameters()\n",
        "    else:\n",
        "        trainable_paras = list(backbone.parameters()) + list(header.parameters())\n",
        "\n",
        "    '''Optim Prepare'''\n",
        "    if OPTIMIZER == 'adam':\n",
        "        optimizer = torch.optim.Adam(trainable_paras, weight_decay=weight_d, lr=lr)\n",
        "    elif OPTIMIZER == 'sgd':\n",
        "        optimizer = torch.optim.SGD(trainable_paras, lr=lr, momentum=momentum, weight_decay=weight_d)\n",
        "    else:\n",
        "        raise ValueError(\"Optimizer not supported.\")\n",
        "\n",
        "    if SCHEDULER == True:\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step, gamma=gamma)\n",
        "    \n",
        "    backbone.train()\n",
        "    header.train()\n",
        "    backbone = backbone.to(device)\n",
        "    header = header.to(device)\n",
        "    best_acc = 80\n",
        "    criterion = torch.nn.BCELoss()\n",
        "    for epoch in range(epoch_num):\n",
        "        train_loss, train_acc = train(train_loader, backbone, header, optimizer, criterion, device, epoch)\n",
        "        loss, acc = valid(valid_loader, backbone, header, criterion, device, epoch)\n",
        "        print('[Training in epoch {:}] loss:{:.3f} acc:{:.3f}'.format(epoch+1, train_loss, train_acc))\n",
        "        print('[Validation in epoch {:}] loss:{:.3f} acc:{:.3f} '.format(epoch+1, loss, acc))\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            check_point(backbone, header, loss, acc, model_dir)\n",
        "        if is_stop(loss, acc):\n",
        "            break\n",
        "        if SCHEDULER == True:\n",
        "            scheduler.step()\n",
        "\n",
        "\n",
        "\"\"\"********************************************* \n",
        "  Training\n",
        " *********************************************\"\"\"\n",
        "backbone = Backbone(preprocessor.embedding_matrix, **net_config)\n",
        "header = Header(**header_config)\n",
        "\n",
        "if mode == 'train':\n",
        "    if WANDB:\n",
        "        wandb.init(project='MLHW4')\n",
        "        wandb_update()\n",
        "    if os.path.isfile(MODEL_DIR + '_backbone_' + CHECKPOINT + '.pth') and os.path.isfile(MODEL_DIR + '_header_' + CHECKPOINT + '.pth'):\n",
        "        print('Loading RNN model...')\n",
        "        backbone = torch.load(MODEL_DIR + '_backbone_' + CHECKPOINT + '.pth')\n",
        "        header = torch.load(MODEL_DIR + '_header_' + CHECKPOINT + '.pth')\n",
        "    run_training(train_loader, valid_loader, backbone, header, EPOCH_NUM, lr, device, MODEL_DIR)\n",
        "\n",
        "\n",
        "\"\"\"********************************************* \n",
        "  Testing\n",
        " *********************************************\"\"\"\n",
        "def run_testing(test_loader, backbone, header, device, output_path):\n",
        "    with open(output_path, 'w') as f:\n",
        "        backbone.eval()\n",
        "        header.eval()\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['id', 'label'])\n",
        "        with torch.no_grad():\n",
        "            for i, (idx_list, lengths, texts) in enumerate(test_loader):\n",
        "                lengths, inputs = lengths.to(device), texts.to(device)\n",
        "                if not backbone is None:\n",
        "                    inputs = backbone(inputs)\n",
        "                soft_predicted = header(inputs, lengths)\n",
        "                hard_predicted = (soft_predicted >= 0.5).int()\n",
        "                for i, p in zip(idx_list, hard_predicted):\n",
        "                    writer.writerow([str(i.item()), str(p.item())])\n",
        "\n",
        "if os.path.isfile(MODEL_DIR + '_backbone_' + CHECKPOINT + '.pth') and os.path.isfile(MODEL_DIR + '_header_' + CHECKPOINT + '.pth'):\n",
        "    print('Loading RNN model...')\n",
        "    backbone = torch.load(MODEL_DIR + '_backbone_' + CHECKPOINT + '.pth')\n",
        "    header = torch.load(MODEL_DIR + '_header_' + CHECKPOINT + '.pth')\n",
        "# pred_file = './drive/MyDrive/Colab Notebooks/MLHW4/testing_result/pred_colab.csv' # In colab version\n",
        "pred_file = './pred_colab.csv'\n",
        "run_testing(test_loader, backbone, header, device, pred_file)\n",
        "from google.colab import files\n",
        "files.download(pred_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This section is for report question 5\n",
        "test_idx, test_text = load_test('./hwq5.csv')\n",
        "test_dataset = TwitterDataset(test_idx, test_text, None, preprocessor)\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE, shuffle = False, collate_fn = test_dataset.collate_fn, num_workers = 8)\n",
        "\n",
        "\n",
        "\"\"\"********************************************* \n",
        "  Definition of RNN network\n",
        " *********************************************\"\"\"\n",
        "class Backbone(torch.nn.Module):\n",
        "    def __init__(self, embedding, embedding_dim, hidden_dim, num_layers, bidirectional, fix_embedding=True):\n",
        "        super(Backbone, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))\n",
        "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
        "        self.embedding.weight.requires_grad = False if fix_embedding else True  # 是否將embedding固定住，如果fix_embedding爲False，在訓練過程中，embedding也會跟着被訓練\n",
        "        \n",
        "        # self.net = torch.nn.RNN(embedding.size(1), hidden_dim, num_layers=num_layers, bidirectional=bidirectional, batch_first=True)\n",
        "        self.net = torch.nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=bidirectional)  # Use config on web\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        inputs = self.embedding(inputs)\n",
        "        x, _ = self.net(inputs)\n",
        "        return x\n",
        "    \n",
        "class Header(torch.nn.Module):\n",
        "    def __init__(self, dropout, hidden_dim):\n",
        "        super(Header, self).__init__()\n",
        "        # TODO: you should design your classifier module\n",
        "        self.classifier = torch.nn.Sequential(torch.nn.Linear(hidden_dim, 1),\n",
        "                            torch.nn.Sigmoid())\n",
        "        \n",
        "    @ torch.no_grad()\n",
        "    def _get_length_masks(self, lengths):\n",
        "        # lengths: (batch_size, ) in cuda\n",
        "        ascending = torch.arange(MAX_POSITIONS_LEN)[:lengths.max().item()].unsqueeze(0).expand(len(lengths), -1).to(lengths.device)\n",
        "        length_masks = (ascending < lengths.unsqueeze(-1)).unsqueeze(-1)\n",
        "        return length_masks\n",
        "    \n",
        "    def forward(self, inputs, lengths):\n",
        "        # the input shape should be (N, L, D∗H)\n",
        "        pad_mask = self._get_length_masks(lengths)\n",
        "        inputs = inputs * pad_mask\n",
        "        inputs = inputs.sum(dim=1)\n",
        "        out = self.classifier(inputs).squeeze()\n",
        "        return out\n",
        "backbone = Backbone(preprocessor.embedding_matrix, **net_config)\n",
        "header = Header(**header_config)\n",
        "\n",
        "def run_testing(test_loader, backbone, header, device, output_path):\n",
        "    with open(output_path, 'w') as f:\n",
        "        backbone.eval()\n",
        "        header.eval()\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['id', 'label'])\n",
        "        with torch.no_grad():\n",
        "            for i, (idx_list, lengths, texts) in enumerate(test_loader):\n",
        "                lengths, inputs = lengths.to(device), texts.to(device)\n",
        "                if not backbone is None:\n",
        "                    inputs = backbone(inputs)\n",
        "                soft_predicted = header(inputs, lengths)\n",
        "                hard_predicted = (soft_predicted >= 0.5).int()\n",
        "                for i, p in zip(idx_list, hard_predicted):\n",
        "                    writer.writerow([str(i.item()), str(p.item())])\n",
        "# MODEL_DIR = './drive/MyDrive/Colab Notebooks/MLHW4/model/RNN/model'\n",
        "# CHECKPOINT = '80.02182'\n",
        "if os.path.isfile(MODEL_DIR + '_backbone_' + CHECKPOINT + '.pth') and os.path.isfile(MODEL_DIR + '_header_' + CHECKPOINT + '.pth'):\n",
        "    print('Loading RNN model...')\n",
        "    backbone = torch.load(MODEL_DIR + '_backbone_' + CHECKPOINT + '.pth')\n",
        "    header = torch.load(MODEL_DIR + '_header_' + CHECKPOINT + '.pth')\n",
        "pred_file = './pred_colab.csv'\n",
        "run_testing(test_loader, backbone, header, device, pred_file)\n",
        "from google.colab import files\n",
        "files.download(pred_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "uUKM-ggHeQZp",
        "outputId": "bd59e865-e7a5-467f-af4b-d685a247cc94"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading RNN model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_61320a31-7c8a-470e-a23f-cca2488409e4\", \"pred_colab.csv\", 25)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "V2cfZDoCfvfk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "a9bc9923-0673-4b15-9028-982c70d7dd18"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4aa6ca60-3fbb-4d6c-9c11-eeb6754daccd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4aa6ca60-3fbb-4d6c-9c11-eeb6754daccd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving w2v_local_250_parse_text.model to w2v_local_250_parse_text (1).model\n",
            "move w2v_local_250_parse_text.model to ./w2v_local_250_parse_text.model\n"
          ]
        }
      ],
      "source": [
        "# import and Create new folder\n",
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# new_folder = '../drive/MyDrive/Colab Notebooks/MLHW4/model/w2v/'  # In colab version\n",
        "new_folder = './'\n",
        "\n",
        "# if os.path.isdir(new_folder):\n",
        "#   shutil.rmtree(new_folder)\n",
        "\n",
        "# os.mkdir(new_folder)\n",
        "\n",
        "# Upload Files\n",
        "uploaded = files.upload()\n",
        "for filename in uploaded.keys():\n",
        "  dst_path = os.path.join(new_folder, filename)\n",
        "  print(f'move {filename} to {dst_path}')\n",
        "  shutil.move(filename, dst_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZAARYspLWPE",
        "outputId": "a65631b1-1f29-40a4-bda5-08da52308998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT2wmdsVDcX6"
      },
      "source": [
        "# Good luck for your programming assignments! \n",
        "If you have any questions, feel free to send e-mails to ntueemlta2022@gmail.com / b08202033@ntu.edu.tw. Of course, welcome to make use of the TA hours as well. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}