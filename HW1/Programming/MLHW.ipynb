{"cells":[{"cell_type":"markdown","metadata":{"id":"mz0_QVkxCrX3"},"source":["# **2022 ML FALL HW1: PM2.5 Prediction (Regression)**"]},{"cell_type":"markdown","metadata":{"id":"ZeZnPAiwDRWG"},"source":["Author: MLTAs\n","\n","Methods:\n","* Training with all data\n","* Training config: mini-batch=512, optimizer=Adam, learning rate=0.1 (TODO: Change the config!)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wS_4-77xHk44"},"source":["# **Import Some Packages**"]},{"cell_type":"code","execution_count":111,"metadata":{"executionInfo":{"elapsed":337,"status":"ok","timestamp":1665021020552,"user":{"displayName":"何秉學","userId":"15307262397001925156"},"user_tz":-480},"id":"k-onQd4JNA5H"},"outputs":[],"source":["import numpy as np\n","import csv\n","import math\n","import pandas as pd\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":112,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":147,"status":"ok","timestamp":1665021021059,"user":{"displayName":"何秉學","userId":"15307262397001925156"},"user_tz":-480},"id":"iUIroiX8jBlb","outputId":"06a07680-c85e-47b1-e740-9cce6f9fba95"},"outputs":[{"data":{"text/plain":["'d:\\\\NTU\\\\First Year\\\\Machine Learning\\\\HW\\\\HW1\\\\Programming'"]},"execution_count":112,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","os.getcwd()"]},{"cell_type":"markdown","metadata":{"id":"aqMEWsRekx0L"},"source":["# **Fix random seed**\n","\n","\n","This is for the reproduction of your result. **DO NOT modify this secton!** \n"]},{"cell_type":"code","execution_count":113,"metadata":{"id":"UxDA6fJb_Uem"},"outputs":[],"source":["import random\n","\n","seed = 9487\n","random.seed(seed)"]},{"cell_type":"markdown","metadata":{"id":"0OVRMuTAc1_E"},"source":["# **Download training data**\n"]},{"cell_type":"code","execution_count":114,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1271,"status":"ok","timestamp":1664347474606,"user":{"displayName":"機器學習","userId":"03249990876179673050"},"user_tz":-480},"id":"s0Zo8JUp5kJ4","outputId":"208abac6-741a-4d9e-8136-7be1da9d3385"},"outputs":[],"source":["# !gdown --id \"1Hfzrcm69QwdFvdeF0uASoQlcVxKw_hHy\" --output \"train.csv\"s\n","# !gdown --id '1FXJztppYG9Q4b_4NHvcmPsc4o5obceWC' --output \"test.csv\"\n","\n","# Incase the links above die, you can use the following instead.\n","#!gdown --id '11abE854Eyv4BA7qt5k8r_80sJ3KuOQUN' --output \"train.csv\"\n","# !gdown --id '1uod-Z4ztluXnuHtgUbm39nMudUKqXHMl' --output \"test.csv\"\n","\n","# If the data is still missing, you can manually download it from kaggle, and upload the files under /content"]},{"cell_type":"markdown","metadata":{},"source":["# **Define function**"]},{"cell_type":"code","execution_count":115,"metadata":{"id":"yHpuZmQwXpz8"},"outputs":[],"source":["def valid(x):\n","    # Total unvalid datas is 35\n","    global count\n","    dic = {\n","        'AMB_TEMP':0, 'CO':1, 'NO':2, 'NO2':3, 'NOx':4, 'O3':5,\n","        'PM10':6, 'WS_HR':7, 'RAINFALL':8, 'RH':9, 'SO2':10,\n","        'WD_HR':11, 'WIND_DIREC':12, 'WIND_SPEED':13, 'PM2.5':14}\n","    for i in range(len(x[0])):\n","        if x[dic['CO']][i] > 1.4 or x[dic['NO']][i] > 40 or x[dic['NO2']][i] > 55 or \\\n","            x[dic['NOx']][i] > 75 or x[dic['O3']][i] > 120 or x[dic['PM10']][i] > 150 or \\\n","            x[dic['RAINFALL']][i] > 30 or x[dic['SO2']][i] > 20 or x[dic['PM2.5']][i] > 65:\n","            count.append(i)\n","    count.reverse()\n","    for i in range(len(count)):\n","        for j in range(15):\n","            del x[j][count[i]]\n","    return np.array(x)\n","\n","# Create your dataset\n","def parse2train_x(data, feats):\n","\n","    x = []\n","\n","    # Use data #0~#7 to predict #8 => Total data length should be decresased by 8.\n","    total_length = data.shape[1] - 8  # data.shape[1] == 5774\n","    \n","    for i in range(total_length):\n","        x_tmp = data[feats, i:i+8] # Use data #0~#7 to predict #8, data #1~#8 to predict #9, etc. Feats is row of data and i:i+8 is column of data\n","        x.append(x_tmp.reshape(-1,))\n","    \n","    # Total valid datas is 5739\n","    # x.shape: (n, 15, 8)\n","    x = np.array(x)\n","    return x\n","\n","def parse2train_y(data, feats):\n","    y = []\n","\n","    # Use data #0~#7 to predict #8 => Total data length should be decresased by 8.\n","    total_length = data.shape[1] - 8  # data.shape[1] == 5774\n","    \n","    for i in range(total_length):\n","        y_tmp = data[-1, i+8] # last column of (i+8)th row: PM2.5\n","        y.append(y_tmp)\n","    \n","    y = np.array(y)\n","    return y\n","    \n","def parse2test(data, feats):\n","    x = []\n","    for i in range(90):\n","        x_tmp = data[feats, 8*i:8*i+8]\n","        x.append(x_tmp.reshape(-1,))\n","\n","    # x.shape: (n, 15, 8)\n","    x = np.array(x)\n","    return x\n","\n","def norm_data(train_data, norm_case, test_data):\n","    dic_data_round = {\n","        0 : 1, 1 : 2, 2 : 1, 3 : 1, 4 : 1,\n","        5 : 1, 6 : 0, 7 : 1, 8 : 1, 9 : 0, \n","        10 : 1, 11 : 0, 12 : 0, 13 : 1, 14 : 0\n","    }\n","\n","    # Concate data\n","    concate_data = np.concatenate((train_data, test_data), axis=1)\n","\n","    # Z-Score\n","    if norm_case == 0:\n","        mean_arr = np.zeros(15)\n","        std_arr = np.zeros(15)\n","\n","        for i in range(15):\n","            # Compute mean\n","            mean_temp = sum(concate_data[i])\n","            mean_arr[i] = mean_temp / float(len(concate_data[i]))\n","        \n","            # Compute std\n","            std_temp = 0\n","            for j in range(len(concate_data[i])):\n","                std_temp += (concate_data[i][j] - mean_arr[i])**2\n","            std_arr[i] = (std_temp / float((len(concate_data[i] - 1))))**0.5\n","\n","            # Create normalize data\n","            train_data[i] -= mean_arr[i]\n","            train_data[i] = np.round(train_data[i] / std_arr[i], dic_data_round[i]+1)\n","            test_data[i] -= mean_arr[i]\n","            test_data[i] = np.round(test_data[i] / std_arr[i], dic_data_round[i]+1)\n","        return train_data, test_data\n","\n","    # Max-Min\n","    elif norm_case == 1:\n","        for i in range(15):\n","            max_temp = max(concate_data[i])\n","            min_temp = min(concate_data[i])\n","            train_data[i] = np.round((train_data[i] - min_temp) / (max_temp - min_temp), dic_data_round[i]+1)\n","            test_data[i] = np.round((test_data[i] - min_temp) / (max_temp - min_temp), dic_data_round[i]+1)\n","        return train_data, test_data\n","    \n","    # MaxAbs\n","    elif norm_case == 2:\n","        for i in range(15):\n","            maxabs_temp = abs(max(concate_data[i]))\n","            train_data[i] = np.round(train_data[i] / maxabs_temp, dic_data_round[i]+1)\n","            test_data[i] = np.round(test_data[i] / maxabs_temp, dic_data_round[i]+1)\n","        return train_data, test_data\n","\n","    # RobustScaler\n","    elif norm_case == 3:\n","\n","        return train_data\n","\n","    else:\n","        print('No define the normalize method, please check again')\n","        return train_data\n","\n","def visul_data(data, path, norm_type=0):\n","    dic_data_title = {\n","        0 : 'AMB_TEMP', 1 : 'CO', 2 : 'NO', 3 : 'NO2', 4 : 'NOx',\n","        5 : 'O3', 6 : 'PM10', 7 : 'WS_HR', 8 : 'RAINFALL', 9 : 'RH',\n","        10 : 'SO2', 11 : 'WD_HR', 12 : 'WIND_DIREC', 13 : 'WIND_SPEED', 14 : 'PM2.5'\n","    }\n","    dic_norm_type = {\n","        0:'_zscore_norm.png',\n","        1:'_maxmin_norm.png',\n","        2:'_maxabs_norm.png',\n","        3:'_robust_norm.png'\n","    }\n","\n","    for i in range(15):\n","        j = np.arange(len(data[i]))\n","        plt.scatter(j, data[i])\n","        plt.xlabel('i-th data')\n","        plt.ylabel(dic_data_title[i])\n","        plt.savefig(path + dic_data_title[i] + dic_norm_type[norm_type])\n","        plt.show()\n"]},{"cell_type":"code","execution_count":116,"metadata":{"id":"XL_RVBoLuXvj"},"outputs":[],"source":["# TODO: Implement 2-nd polynomial regression version for the report.\n","def minibatch(x, y, config):\n","  \n","    # Randomize the data in minibatch\n","    index = np.arange(x.shape[0])\n","    np.random.shuffle(index)\n","    x = x[index]\n","    y = y[index]\n","    \n","    # Initialization\n","    batch_size = config.batch_size\n","    lr = config.lr\n","    lam = config.lam\n","    epoch = config.epoch\n","\n","    beta_1 = np.full(x[0].shape, 0.9).reshape(-1, 1)\n","    beta_2 = np.full(x[0].shape, 0.99).reshape(-1, 1)\n","    # Linear regression: only contains two parameters (w, b).\n","    w = np.full(x[0].shape, 0.1).reshape(-1, 1)\n","    bias = 0.1\n","    m_t = np.full(x[0].shape, 0).reshape(-1, 1)\n","    v_t = np.full(x[0].shape, 0).reshape(-1, 1)\n","    m_t_b = 0.0\n","    v_t_b = 0.0\n","    t = 0\n","    epsilon = 1e-8\n","    \n","    # Training loop\n","    total_loss = np.zeros(epoch)\n","    for num in range(epoch):\n","        for b in range(int(x.shape[0]/batch_size)):\n","            t+=1\n","            x_batch = x[b * batch_size:(b+1) * batch_size]\n","            y_batch = y[b * batch_size:(b+1) * batch_size].reshape(-1,1)\n","\n","            # Prediction of linear regression \n","            pred = np.dot(x_batch, w) + bias\n","            # loss\n","            loss = y_batch - pred\n","            # total_loss[num] += (loss**2).sum(axis=0)\n","            \n","            # Compute gradient\n","            g_t = np.dot(x_batch.transpose(), loss) * (-2) +  2 * lam * np.sum(w) \n","            g_t_b = loss.sum(axis=0) * (-2)\n","            m_t = beta_1 * m_t + (1-beta_1) * g_t \n","            v_t = beta_2 * v_t + (1-beta_2) * np.multiply(g_t, g_t)\n","            m_cap = m_t / (1-(beta_1**t))\n","            v_cap = v_t / (1 - (beta_2**t))\n","            m_t_b = 0.9 * m_t_b + (1 - 0.9) * g_t_b\n","            v_t_b = 0.99 * v_t_b + (1 - 0.99) * (g_t_b * g_t_b) \n","            m_cap_b = m_t_b / (1 - (0.9**t))\n","            v_cap_b = v_t_b / (1 - (0.99**t))\n","            w_0 = np.copy(w)\n","            \n","            # Update weight & bias\n","            w -= ((lr * m_cap) / (np.sqrt(v_cap) + epsilon)).reshape(-1, 1)\n","            bias -= (lr * m_cap_b) / (math.sqrt(v_cap_b) + epsilon)\n","            \n","        # total_loss[num] /= x.shape[0]\n","        \n","    # plt.plot(total_loss)    \n","    # plt.show()\n","    return w, bias"]},{"cell_type":"code","execution_count":117,"metadata":{"id":"ZpdOsMfXLxH2"},"outputs":[],"source":["from argparse import Namespace\n","\n","# TODO: Tune the config to boost your performance. \n","train_config = Namespace(\n","    batch_size = 512,\n","    lr = 1e-1,\n","    lam = 0.001,\n","    epoch = 100,\n",")"]},{"cell_type":"markdown","metadata":{"id":"ay-RhqqA88vS"},"source":["# **Training your regression model**"]},{"cell_type":"code","execution_count":118,"metadata":{"id":"_Akqj5yYVGHA"},"outputs":[],"source":["# Choose your features to train. \n","# Hint: \n","# 1. You can select more than one feature.\n","# 2. You should select \"good\" features.\n","\n","# TODO: Carefully justify which feature should be chosen.\n","# feats = [1, 2, 3, 4, 6, 14] # Choose CO, NO, NO2, NOx, PM2.5, PM10\n","# feats = [0, 5, 7, 8, 9, 10, 11, 12, 13]\n","# feats = np.ndarray.tolist(np.arange(15))\n","feats = [1, 2, 3, 4, 6, 7, 8, 9, 13, 14]"]},{"cell_type":"code","execution_count":119,"metadata":{"id":"AiEWGMQXLM99"},"outputs":[],"source":["# Data preprocessing.\n","train_data = pd.read_csv(\"./dataset/train.csv\")\n","train_data = train_data.values  # Data type is <class 'numpy.ndarray'>\n","\n","# To transpose the data to data^T\n","train_data = np.transpose(np.array(np.float64(train_data)))   # Data type is <class 'numpy.ndarray'>\n","\n","# Filter valid train data\n","count = []\n","train_data = np.ndarray.tolist(train_data)\n","train_data = valid(train_data)\n","\n","# Split train_y data before normalization\n","train_x = parse2train_x(train_data, feats)\n","train_y = parse2train_y(train_data, feats)\n","\n","\n","# Normalize data\n","# train_data, test_data_ = norm_data(train_data, 0, test_data)\n","\n","\n","test_data = pd.read_csv('./dataset/test.csv')\n","test_data = test_data.values\n","test_data = np.transpose(np.array(np.float64(test_data)))\n","test_x = parse2test(test_data, feats)"]},{"cell_type":"markdown","metadata":{"id":"WyEpvVVQdZ0c"},"source":["# **Adam**\n","* This is our gradient descent algorithm. Adam was implemented.\n","* You can implement another algorithm such as SGD, which may (or may not) boost the performance.\n","* However, **modules like sklearn and pytorch are not allowed**.\n","* Ref: https://arxiv.org/pdf/1412.6980.pdf\n","![](https://i.imgur.com/jRaebdf.png)\n","\n"]},{"cell_type":"code","execution_count":120,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1664347475285,"user":{"displayName":"機器學習","userId":"03249990876179673050"},"user_tz":-480},"id":"HhfoPJUhcnH9","outputId":"5add5fc3-afa9-4ec3-a897-0e1f97060374"},"outputs":[{"name":"stdout","output_type":"stream","text":["(88, 1) (1,)\n"]}],"source":["# Train your regression model\n","w, bias = minibatch(train_x, train_y, train_config)\n","print(w.shape, bias.shape)"]},{"cell_type":"markdown","metadata":{"id":"fWrfEwaEdO6J"},"source":["# **Testing & Write result as .csv**\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":121,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":293,"status":"ok","timestamp":1664347481053,"user":{"displayName":"機器學習","userId":"03249990876179673050"},"user_tz":-480},"id":"TqEQ1fZ9-WMO","outputId":"ed060dcc-8be6-4278-9466-bb632ac66a9c"},"outputs":[{"name":"stdout","output_type":"stream","text":["(90, 88)\n"]}],"source":["with open('./testing_result/my_sol.csv', 'w', newline='') as csvf:\n","    # 建立 CSV 檔寫入器\n","    writer = csv.writer(csvf)\n","    writer.writerow(['Id','Predicted'])\n","\n","    print(test_x.shape) \n","    for i in range(int(test_x.shape[0])):\n","      # Prediction of linear regression \n","      prediction = (np.dot(np.reshape(w,-1),test_x[i]) + bias)[0]\n","      writer.writerow([i, prediction] )"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[{"file_id":"1ffy7uvNJr_rtaNbIlGZdrY47TkjzRKx-","timestamp":1665022477970},{"file_id":"1F-E9mHTJXo4heU8tr72UVIiHXR4MNpQi","timestamp":1663000203559},{"file_id":"1feslucFrEPkkuEJyFmPjEiaEhSyKAAmG","timestamp":1662890865314},{"file_id":"1Xj7rpNfdiYwq0R7udY_p13N_2PJfDd8k","timestamp":1662888507120}]},"kernelspec":{"display_name":"Python 3.8.13 ('NTU_ML')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"a04df75c32766f3d499272b3c4b6cc9162998ee7afe31618e8fd853f9d59c375"}}},"nbformat":4,"nbformat_minor":0}
