{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OC_P8bFMzILD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Bernie\\anaconda3\\envs\\NTU_ML\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.optim import SGD\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "seed = 3047\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Iywj8wLF8ppU"
      },
      "outputs": [],
      "source": [
        "class SVM(nn.Module):\n",
        "  def __init__(self):\n",
        "    # TODO design your model\n",
        "    super(SVM, self).__init__() \n",
        "    self.w = nn.Parameter(torch.randn((1, 32)).to(torch.float32))\n",
        "    self.f = nn.Sequential(\n",
        "                  nn.Linear(107, 32),\n",
        "                  nn.Dropout(0.5),\n",
        "                )\n",
        "  def transform(self, x):\n",
        "    x = self.f(x)\n",
        "    return x\n",
        "  def kernel(self, x):\n",
        "    pass\n",
        "  def forward(self, x):\n",
        "    f = torch.matmul(self.transform(x), self.w.T)\n",
        "    \n",
        "    return f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "X2a0522z7IWo"
      },
      "outputs": [],
      "source": [
        "class HingeLoss(nn.Module):\n",
        "  def __init__(self, C):\n",
        "    super(HingeLoss, self).__init__()  \n",
        "    self.C = C\n",
        "  def forward(self, y, f):\n",
        "    loss = 0\n",
        "    for i in range(len(y)):\n",
        "      loss = loss + max(0, 1-y[i]*f[i])  # define Hinge loss\n",
        "    loss = loss * self.C\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iBTrbIzE_AI9"
      },
      "outputs": [],
      "source": [
        "def cal_mu_std():\n",
        "  X = pd.read_csv(\"./dataset/train.csv\")\n",
        "  mu = X.drop(['y'], axis=1).mean() # The mean of whole features except label y\n",
        "  std = X.drop(['y'], axis=1).std() # The std of whole features except label y\n",
        "\n",
        "  return mu, std\n",
        "\n",
        "class TrainDataset(Dataset):\n",
        "  def __init__(self, split, mu=None, std=None):\n",
        "    X = pd.read_csv(f\"{split}.csv\")\n",
        "    \n",
        "    Y = X['y'].values.reshape(-1) * 2 - 1\n",
        "    self.mu, self.std = mu, std\n",
        "    # print(mu, std)\n",
        "    X = self.normalize(X.drop(['y'], axis=1), self.mu, self.std)\n",
        "    X = np.concatenate((X, np.ones((X.shape[0], 1))), 1)\n",
        "    self.Y = torch.from_numpy(Y).to(torch.float32)\n",
        "    self.X = torch.from_numpy(X).to(torch.float32)\n",
        "\n",
        "  def normalize(self, X, mu=None, std=None):\n",
        "    X = (X-mu)/std\n",
        "    \n",
        "    return X\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.X.size(0)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.Y[idx]\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "  def __init__(self, mu, std):\n",
        "    X = pd.read_csv(\"./dataset/X_test\")\n",
        "    X = self.normalize(X, mu, std)\n",
        "    X = np.concatenate((X, np.ones((X.shape[0], 1))), 1)\n",
        "    self.X = torch.from_numpy(X).to(torch.float32)\n",
        "\n",
        "  def normalize(self, X, mu_x, std_x):\n",
        "    X = (X-mu_x)/std_x\n",
        "    \n",
        "    return X\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.X.size(0)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m5PZP044Pxuu"
      },
      "outputs": [],
      "source": [
        "def train(train_data, val_data, model, optim, C, device='cuda:0'):\n",
        "    epoch = 100\n",
        "    objective = HingeLoss(C)\n",
        "    steps = 0\n",
        "    best = 0\n",
        "\n",
        "    for e in range(epoch):\n",
        "      train_total_loss = 0\n",
        "      for tr in train_data:\n",
        "        steps += 1\n",
        "        x_train, y_train = tr\n",
        "        x_train, y_train = x_train.to(device), y_train.to(device)\n",
        "        pred = model(x_train).squeeze(1)\n",
        "        loss = objective(pred, y_train) + 1 / 2 * torch.sum(model.w[:-1] ** 2)\n",
        "        \n",
        "        optim.zero_grad()\n",
        "        loss.backward() #retain_graph=True\n",
        "        optim.step()\n",
        "\n",
        "        train_total_loss += (loss.item() / len(train_data))\n",
        "\n",
        "        if steps % 100 == 0:\n",
        "          model.eval()\n",
        "          with torch.no_grad():\n",
        "            acc = []\n",
        "            for val in val_data:\n",
        "              x_val, y_val = val\n",
        "              x_val , y_val = x_val.to(device), y_val.to(device)\n",
        "              pred = model(x_val).squeeze(1)\n",
        "              pred = (pred > 0) * 2 - 1\n",
        "              \n",
        "              result = (y_val == pred)\n",
        "              acc += [(float(result.sum()) / result.size(0))]\n",
        "            acc = sum(acc) / len(acc)\n",
        "            print(f'Steps {steps}| Train Loss = {train_total_loss}| Val acc = {acc}')\n",
        "            if acc > best:\n",
        "              torch.save(model.state_dict(), './model/best_handcraft.ckpt')\n",
        "              best = acc        \n",
        "          model.train()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ucnbZEiVPia3"
      },
      "outputs": [],
      "source": [
        "lr = 0.01\n",
        "batch = 32\n",
        "C = 1\n",
        "device = 'cuda:0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YF1QyZXG4eI7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Steps 100| Train Loss = 4.254057230371418| Val acc = 0.8136666666666666\n",
            "Steps 200| Train Loss = 6.514630773882849| Val acc = 0.8296666666666667\n",
            "Steps 300| Train Loss = 8.187162278018475| Val acc = 0.8356666666666667\n",
            "Steps 400| Train Loss = 10.377509416181812| Val acc = 0.8366666666666667\n",
            "Steps 500| Train Loss = 12.141175177448241| Val acc = 0.826\n",
            "Steps 600| Train Loss = 13.998730886808211| Val acc = 0.8223333333333334\n",
            "Steps 700| Train Loss = 15.976334617767511| Val acc = 0.8116666666666666\n",
            "Steps 800| Train Loss = 18.100007889848776| Val acc = 0.837\n",
            "Steps 900| Train Loss = 20.24538290113599| Val acc = 0.7943333333333333\n",
            "Steps 1000| Train Loss = 1.5157028684368377| Val acc = 0.8183333333333334\n",
            "Steps 1100| Train Loss = 3.500360795687804| Val acc = 0.822\n",
            "Steps 1200| Train Loss = 5.8324105050656705| Val acc = 0.7996666666666666\n",
            "Steps 1300| Train Loss = 7.9660625981562045| Val acc = 0.8176666666666667\n",
            "Steps 1400| Train Loss = 10.00904294448497| Val acc = 0.808\n",
            "Steps 1500| Train Loss = 12.167982626528968| Val acc = 0.8366666666666667\n",
            "Steps 1600| Train Loss = 14.196311718973758| Val acc = 0.806\n",
            "Steps 1700| Train Loss = 16.39113721651432| Val acc = 0.822\n",
            "Steps 1800| Train Loss = 19.2493059980921| Val acc = 0.81\n",
            "Steps 1900| Train Loss = 1.3386071997803524| Val acc = 0.8103333333333333\n",
            "Steps 2000| Train Loss = 3.761091093222299| Val acc = 0.8123333333333334\n",
            "Steps 2100| Train Loss = 5.787273526449741| Val acc = 0.8363333333333334\n",
            "Steps 2200| Train Loss = 8.860692429078089| Val acc = 0.8303333333333334\n",
            "Steps 2300| Train Loss = 11.493126670777533| Val acc = 0.8203333333333334\n",
            "Steps 2400| Train Loss = 13.911347583258816| Val acc = 0.8313333333333334\n",
            "Steps 2500| Train Loss = 16.845549049315512| Val acc = 0.825\n",
            "Steps 2600| Train Loss = 19.72773470352223| Val acc = 0.7986666666666666\n",
            "Steps 2700| Train Loss = 22.712582026725205| Val acc = 0.8196666666666667\n",
            "Steps 2800| Train Loss = 0.9782569563234006| Val acc = 0.811\n",
            "Steps 2900| Train Loss = 3.62631591780361| Val acc = 0.8166666666666667\n",
            "Steps 3000| Train Loss = 7.434862024340276| Val acc = 0.8213333333333334\n",
            "Steps 3100| Train Loss = 10.469180057575176| Val acc = 0.8323333333333334\n",
            "Steps 3200| Train Loss = 13.060265753047288| Val acc = 0.818\n",
            "Steps 3300| Train Loss = 16.52004773934166| Val acc = 0.7723333333333333\n",
            "Steps 3400| Train Loss = 20.08488654277542| Val acc = 0.8246666666666667\n",
            "Steps 3500| Train Loss = 22.797063961044532| Val acc = 0.8146666666666667\n",
            "Steps 3600| Train Loss = 26.319978181288853| Val acc = 0.8083333333333333\n",
            "Steps 3700| Train Loss = 0.10485487892514184| Val acc = 0.8306666666666667\n",
            "Steps 3800| Train Loss = 2.8617000053455293| Val acc = 0.8233333333333334\n",
            "Steps 3900| Train Loss = 6.0354712280360125| Val acc = 0.8213333333333334\n",
            "Steps 4000| Train Loss = 10.879417713844415| Val acc = 0.824\n",
            "Steps 4100| Train Loss = 14.516917839968874| Val acc = 0.8246666666666667\n",
            "Steps 4200| Train Loss = 17.63673012932658| Val acc = 0.785\n",
            "Steps 4300| Train Loss = 21.043976385098013| Val acc = 0.8003333333333333\n",
            "Steps 4400| Train Loss = 24.892468532700576| Val acc = 0.79\n",
            "Steps 4500| Train Loss = 28.40081110119304| Val acc = 0.8263333333333334\n",
            "Steps 4600| Train Loss = 31.566061003641654| Val acc = 0.8173333333333334\n",
            "Steps 4700| Train Loss = 3.77998099647043| Val acc = 0.8096666666666666\n",
            "Steps 4800| Train Loss = 8.0427794152008| Val acc = 0.8283333333333334\n",
            "Steps 4900| Train Loss = 12.46284382271044| Val acc = 0.825\n",
            "Steps 5000| Train Loss = 16.0057081355677| Val acc = 0.831\n",
            "Steps 5100| Train Loss = 18.98070905425331| Val acc = 0.8183333333333334\n",
            "Steps 5200| Train Loss = 23.057303113854804| Val acc = 0.8256666666666667\n",
            "Steps 5300| Train Loss = 26.143336231058285| Val acc = 0.8403333333333334\n",
            "Steps 5400| Train Loss = 29.67756697548416| Val acc = 0.8203333333333334\n",
            "Steps 5500| Train Loss = 39.3512237280994| Val acc = 0.809\n",
            "Steps 5600| Train Loss = 2.7214558330965253| Val acc = 0.8186666666666667\n",
            "Steps 5700| Train Loss = 7.733549895224635| Val acc = 0.8126666666666666\n",
            "Steps 5800| Train Loss = 12.909008525666739| Val acc = 0.8326666666666667\n",
            "Steps 5900| Train Loss = 17.468948751300967| Val acc = 0.822\n",
            "Steps 6000| Train Loss = 22.23250612087582| Val acc = 0.8316666666666667\n",
            "Steps 6100| Train Loss = 26.718511287010088| Val acc = 0.8043333333333333\n",
            "Steps 6200| Train Loss = 32.48079942495793| Val acc = 0.8223333333333334\n",
            "Steps 6300| Train Loss = 37.1530105538699| Val acc = 0.823\n",
            "Steps 6400| Train Loss = 42.09163353066427| Val acc = 0.8083333333333333\n",
            "Steps 6500| Train Loss = 0.8151707945964036| Val acc = 0.801\n",
            "Steps 6600| Train Loss = 6.070589383701224| Val acc = 0.8036666666666666\n",
            "Steps 6700| Train Loss = 11.975236250982654| Val acc = 0.825\n",
            "Steps 6800| Train Loss = 16.004605291467723| Val acc = 0.8203333333333334\n",
            "Steps 6900| Train Loss = 22.77120236736357| Val acc = 0.794\n",
            "Steps 7000| Train Loss = 27.26875409135574| Val acc = 0.8293333333333334\n",
            "Steps 7100| Train Loss = 33.1733202338219| Val acc = 0.8203333333333334\n",
            "Steps 7200| Train Loss = 37.68681750030488| Val acc = 0.822\n",
            "Steps 7300| Train Loss = 43.91911915867105| Val acc = 0.801\n",
            "Steps 7400| Train Loss = 0.1522777421133859| Val acc = 0.8363333333333334\n",
            "Steps 7500| Train Loss = 7.028221339374395| Val acc = 0.824\n",
            "Steps 7600| Train Loss = 12.795283109594736| Val acc = 0.806\n",
            "Steps 7700| Train Loss = 18.391814092795048| Val acc = 0.816\n",
            "Steps 7800| Train Loss = 23.937913219392037| Val acc = 0.8286666666666667\n",
            "Steps 7900| Train Loss = 30.173807069594716| Val acc = 0.819\n",
            "Steps 8000| Train Loss = 35.55389174212623| Val acc = 0.8213333333333334\n",
            "Steps 8100| Train Loss = 41.11261093745495| Val acc = 0.8336666666666667\n",
            "Steps 8200| Train Loss = 46.841085782556796| Val acc = 0.8256666666666667\n",
            "Steps 8300| Train Loss = 51.277272224168186| Val acc = 0.8136666666666666\n",
            "Steps 8400| Train Loss = 4.224712654109641| Val acc = 0.826\n",
            "Steps 8500| Train Loss = 8.920459978250197| Val acc = 0.8166666666666667\n",
            "Steps 8600| Train Loss = 13.085865982425158| Val acc = 0.826\n",
            "Steps 8700| Train Loss = 17.85807188377752| Val acc = 0.819\n",
            "Steps 8800| Train Loss = 21.73542175561318| Val acc = 0.805\n",
            "Steps 8900| Train Loss = 28.275882298812196| Val acc = 0.7456666666666667\n",
            "Steps 9000| Train Loss = 35.007455609061495| Val acc = 0.82\n",
            "Steps 9100| Train Loss = 42.55102172868075| Val acc = 0.8046666666666666\n",
            "Steps 9200| Train Loss = 49.312329094131236| Val acc = 0.81\n",
            "Steps 9300| Train Loss = 3.737385431925456| Val acc = 0.8103333333333333\n",
            "Steps 9400| Train Loss = 11.355045641655531| Val acc = 0.808\n",
            "Steps 9500| Train Loss = 21.038416680835542| Val acc = 0.8016666666666666\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32md:\\NTU\\First Year\\Machine Learning\\HW\\HW5\\Programming\\handcraft.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NTU/First%20Year/Machine%20Learning/HW/HW5/Programming/handcraft.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NTU/First%20Year/Machine%20Learning/HW/HW5/Programming/handcraft.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m optim \u001b[39m=\u001b[39m SGD(model\u001b[39m.\u001b[39mparameters(), lr)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/NTU/First%20Year/Machine%20Learning/HW/HW5/Programming/handcraft.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m model \u001b[39m=\u001b[39m train(train_dataloader, val_dataloader, model, optim, C, device)\n",
            "\u001b[1;32md:\\NTU\\First Year\\Machine Learning\\HW\\HW5\\Programming\\handcraft.ipynb Cell 7\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_data, val_data, model, optim, C, device)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NTU/First%20Year/Machine%20Learning/HW/HW5/Programming/handcraft.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mfor\u001b[39;00m val \u001b[39min\u001b[39;00m val_data:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NTU/First%20Year/Machine%20Learning/HW/HW5/Programming/handcraft.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m   x_val, y_val \u001b[39m=\u001b[39m val\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/NTU/First%20Year/Machine%20Learning/HW/HW5/Programming/handcraft.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m   x_val , y_val \u001b[39m=\u001b[39m x_val\u001b[39m.\u001b[39;49mto(device), y_val\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NTU/First%20Year/Machine%20Learning/HW/HW5/Programming/handcraft.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m   pred \u001b[39m=\u001b[39m model(x_val)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NTU/First%20Year/Machine%20Learning/HW/HW5/Programming/handcraft.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m   pred \u001b[39m=\u001b[39m (pred \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "mu, std = cal_mu_std()\n",
        "trainset = TrainDataset('./dataset/train', mu, std)\n",
        "devset = TrainDataset('./dataset/val', mu, std)\n",
        "testset = TestDataset(mu, std)\n",
        "\n",
        "train_dataloader = DataLoader(trainset, batch, True, drop_last=False)\n",
        "val_dataloader = DataLoader(devset, 1, False)\n",
        "test_dataloader = DataLoader(testset, 1, False)\n",
        "\n",
        "model = SVM().to(device)\n",
        "model.train()\n",
        "optim = SGD(model.parameters(), lr)\n",
        "model = train(train_dataloader, val_dataloader, model, optim, C, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42J0DE2DQQ8u"
      },
      "outputs": [],
      "source": [
        "best_model = model\n",
        "best_model.load_state_dict(torch.load('./model/best_handcraft.ckpt'))\n",
        "best_model = best_model.eval()\n",
        "\n",
        "y_test = []\n",
        "for x in test_dataloader:\n",
        "  x = x.to(device)\n",
        "  y = best_model(x)\n",
        "  y_test.append(((y > 0) * 1).item())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYJnjxOiQKqB"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "with open('./testing_result/predict_handcraft.csv', 'w', newline='') as csvf:\n",
        "    # 建立 CSV 檔寫入器\n",
        "    writer = csv.writer(csvf)\n",
        "    writer.writerow(['id','label'])\n",
        "    for i in range(len(y_test)):\n",
        "      writer.writerow( [i + 1, int(y_test[i])] )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "NTU_ML",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "a04df75c32766f3d499272b3c4b6cc9162998ee7afe31618e8fd853f9d59c375"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
