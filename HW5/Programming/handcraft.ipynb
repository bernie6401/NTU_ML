{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "OC_P8bFMzILD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.optim import SGD\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "seed = 3047\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "uG8PitZ1fwg2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function wandb.sdk.wandb_login.login(anonymous: Union[Literal['must', 'allow', 'never'], NoneType] = None, key: Union[str, NoneType] = None, relogin: Union[bool, NoneType] = None, host: Union[str, NoneType] = None, force: Union[bool, NoneType] = None, timeout: Union[int, NoneType] = None) -> bool>"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# !pip install wandb\n",
        "import wandb\n",
        "wandb.login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "ucnbZEiVPia3"
      },
      "outputs": [],
      "source": [
        "epoch = 100\n",
        "lr = 0.01\n",
        "batch = 32\n",
        "OPTIMIZER = 'SGD'\n",
        "\n",
        "CHECKPOINT = None\n",
        "weight_d = 1e-3\n",
        "momentum = 0.9\n",
        "gamma = 0.8\n",
        "step = 20\n",
        "\n",
        "data_norm = True\n",
        "SCHEDULER = False\n",
        "\n",
        "C = 1\n",
        "device = 'cuda:0'\n",
        "WANDB = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "ep_na4MMgJGO"
      },
      "outputs": [],
      "source": [
        "\"\"\"********************************************* \n",
        "  Self-defined\n",
        " *********************************************\"\"\"\n",
        "def wandb_update():\n",
        "    config = wandb.config\n",
        "    config.epochs = epoch\n",
        "    config.learning_rate = lr\n",
        "    config.batch_size = batch\n",
        "    config.optimizer = OPTIMIZER\n",
        "\n",
        "    config.checkpoint = CHECKPOINT\n",
        "    config.weight_d = weight_d\n",
        "    config.momentum = momentum\n",
        "    config.gamma = gamma\n",
        "    config.step = step\n",
        "\n",
        "    config.data_norm = data_norm\n",
        "    config.scheduler = SCHEDULER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "FucrkOKI3m_c"
      },
      "outputs": [],
      "source": [
        "# !gdown 1o0m3jyfmetUOJ146TqHuEGUWwQyC7JXV\n",
        "# !gdown 1B5OC3R0yM8F7yjoYOKu3t08QZalcr7DC\n",
        "# !gdown 1THvOuf_EOn6c_6TLy0Bqs23BP2NraBR2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Iywj8wLF8ppU"
      },
      "outputs": [],
      "source": [
        "class SVM(nn.Module):\n",
        "  def __init__(self):\n",
        "    # TODO design your model\n",
        "    super(SVM, self).__init__() \n",
        "    self.w = nn.Parameter(torch.randn((1, 1024)).to(torch.float32))\n",
        "    self.f = nn.Sequential(\n",
        "                  nn.Linear(107, 1024),\n",
        "                  nn.Dropout(0.5),\n",
        "                )\n",
        "  def transform(self, x):\n",
        "    x = self.f(x)\n",
        "    return x\n",
        "  def kernel(self, x):\n",
        "    pass\n",
        "  def forward(self, x):\n",
        "    f = torch.matmul(self.transform(x), self.w.T)\n",
        "    \n",
        "    return f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "X2a0522z7IWo"
      },
      "outputs": [],
      "source": [
        "class HingeLoss(nn.Module):\n",
        "  def __init__(self, C):\n",
        "    super(HingeLoss, self).__init__()  \n",
        "    self.C = C\n",
        "  def forward(self, y, f):\n",
        "    loss = 0\n",
        "    for i in range(len(y)):\n",
        "      loss = loss + max(0, 1-y[i]*f[i])  # define Hinge loss\n",
        "    loss = loss * self.C\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "iBTrbIzE_AI9"
      },
      "outputs": [],
      "source": [
        "def cal_mu_std():\n",
        "  X = pd.read_csv(\"./dataset/train.csv\")\n",
        "  mu = X.drop(['y'], axis=1).mean() # The mean of whole features except label y\n",
        "  std = X.drop(['y'], axis=1).std() # The std of whole features except label y\n",
        "\n",
        "  return mu, std\n",
        "\n",
        "class TrainDataset(Dataset):\n",
        "  def __init__(self, split, mu=None, std=None):\n",
        "    X = pd.read_csv(f\"{split}.csv\")\n",
        "    \n",
        "    Y = X['y'].values.reshape(-1) * 2 - 1\n",
        "    self.mu, self.std = mu, std\n",
        "    if data_norm:\n",
        "      X = self.normalize(X.drop(['y'], axis=1), self.mu, self.std)\n",
        "    X = np.concatenate((X, np.ones((X.shape[0], 1))), 1)\n",
        "    self.Y = torch.from_numpy(Y).to(torch.float32)\n",
        "    self.X = torch.from_numpy(X).to(torch.float32)\n",
        "\n",
        "  def normalize(self, X, mu=None, std=None):\n",
        "    continuous_feat = [\"age\", \"fnlwgt\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]\n",
        "    # X[0] = (X-mu)/std\n",
        "    for i in range(len(continuous_feat)):\n",
        "       X[continuous_feat[i]] = (X[continuous_feat[i]]-mu[continuous_feat[i]])/std[continuous_feat[i]]\n",
        "    \n",
        "    return X\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.X.size(0)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.Y[idx]\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "  def __init__(self, mu, std):\n",
        "    X = pd.read_csv(\"./dataset/X_test\")\n",
        "    if data_norm:\n",
        "      X = self.normalize(X, mu, std)\n",
        "    X = np.concatenate((X, np.ones((X.shape[0], 1))), 1)\n",
        "    self.X = torch.from_numpy(X).to(torch.float32)\n",
        "\n",
        "  def normalize(self, X, mu_x, std_x):\n",
        "    # X = (X-mu_x)/std_x\n",
        "    continuous_feat = [\"age\", \"fnlwgt\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]\n",
        "    for i in range(len(continuous_feat)):\n",
        "       X[continuous_feat[i]] = (X[continuous_feat[i]]-mu_x[continuous_feat[i]])/std_x[continuous_feat[i]]\n",
        "\n",
        "    return X\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.X.size(0)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "m5PZP044Pxuu"
      },
      "outputs": [],
      "source": [
        "def train(train_data, val_data, model, optim, C, device='cuda:0', epoch=None):\n",
        "    objective = HingeLoss(C)\n",
        "    steps = 0\n",
        "    best = 0\n",
        "\n",
        "    for e in range(epoch):\n",
        "      train_total_loss = 0\n",
        "      for tr in train_data:\n",
        "        steps += 1\n",
        "        x_train, y_train = tr\n",
        "        x_train, y_train = x_train.to(device), y_train.to(device)\n",
        "        pred = model(x_train).squeeze(1)\n",
        "        loss = objective(pred, y_train) + 1 / 2 * torch.sum(model.w[:-1] ** 2)\n",
        "        \n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        train_total_loss += (loss.item() / len(train_data))\n",
        "        \n",
        "        if steps % 1000 == 0:\n",
        "          model.eval()\n",
        "          with torch.no_grad():\n",
        "            acc = []\n",
        "            for val in val_data:\n",
        "              x_val, y_val = val\n",
        "              x_val , y_val = x_val.to(device), y_val.to(device)\n",
        "              pred = model(x_val).squeeze(1)\n",
        "              pred = (pred > 0) * 2 - 1\n",
        "              \n",
        "              result = (y_val == pred)\n",
        "              acc += [(float(result.sum()) / result.size(0))]\n",
        "            acc = sum(acc) / len(acc)\n",
        "            print(f'Steps {steps}| Train Loss = {train_total_loss}| Val acc = {acc}')\n",
        "            if acc > best:\n",
        "              torch.save(model.state_dict(), './model/best_handcraft.ckpt')\n",
        "              best = acc\n",
        "          model.train()\n",
        "          \n",
        "          if WANDB:\n",
        "            wandb.log({\"lr\": optim.param_groups[0]['lr'],\n",
        "                        \"train_acc\": acc,\n",
        "                        \"train_loss\": train_total_loss})                   \n",
        "      if SCHEDULER == True:\n",
        "        scheduler.step()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YF1QyZXG4eI7",
        "outputId": "89e1cac8-ace7-43cc-e548-8e57db964fe6"
      },
      "outputs": [],
      "source": [
        "mu, std = cal_mu_std()\n",
        "trainset = TrainDataset('./dataset/train', mu, std)\n",
        "devset = TrainDataset('./dataset/val', mu, std)\n",
        "testset = TestDataset(mu, std)\n",
        "\n",
        "train_dataloader = DataLoader(trainset, batch, True, drop_last=False)\n",
        "val_dataloader = DataLoader(devset, 1, False)\n",
        "test_dataloader = DataLoader(testset, 1, False)\n",
        "\n",
        "model = SVM().to(device)\n",
        "model.train()\n",
        "'''Optim Prepare'''\n",
        "if OPTIMIZER == 'adam':\n",
        "    optim = torch.optim.Adam(model.parameters(), weight_decay=weight_d, lr=lr)\n",
        "elif OPTIMIZER == 'SGD':\n",
        "    optim = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "else:\n",
        "    raise ValueError(\"Optimizer not supported.\")\n",
        "if WANDB:\n",
        "  wandb.init(project='MLHW5')\n",
        "  wandb_update()\n",
        "if SCHEDULER == True:\n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=step, gamma=gamma)\n",
        "model = train(train_dataloader, val_dataloader, model, optim, C, device, epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "42J0DE2DQQ8u"
      },
      "outputs": [],
      "source": [
        "best_model = model\n",
        "best_model.load_state_dict(torch.load('./model/best_handcraft.ckpt'))\n",
        "best_model = best_model.eval()\n",
        "\n",
        "y_test = []\n",
        "for x in test_dataloader:\n",
        "  x = x.to(device)\n",
        "  y = best_model(x)\n",
        "  y_test.append(((y > 0) * 1).item())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "sYJnjxOiQKqB"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "with open('./testing_result/predict_handcraft.csv', 'w', newline='') as csvf:\n",
        "    # 建立 CSV 檔寫入器\n",
        "    writer = csv.writer(csvf)\n",
        "    writer.writerow(['id','label'])\n",
        "    for i in range(len(y_test)):\n",
        "      writer.writerow( [i + 1, int(y_test[i])] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "W314yk12LGnv"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "3HqNcqZfM0RG"
      },
      "outputs": [],
      "source": [
        "# ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "dK3gap39NRsS"
      },
      "outputs": [],
      "source": [
        "# cd 'MLHW5'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "NTU_ML",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "a04df75c32766f3d499272b3c4b6cc9162998ee7afe31618e8fd853f9d59c375"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
